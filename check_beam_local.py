"""An example that writes to and reads from Kafka. This example reads from the PubSub NYC Taxi stream described in https://github.com/googlecodelabs/cloud-dataflow-nyc-taxi-tycoon, writes to a given Kafka topic and reads back from the same Kafka topic. """import loggingimport sysimport apache_beam as beamfrom apache_beam.io.kafka import ReadFromKafkafrom apache_beam.io import WriteToBigQueryfrom apache_beam.options.pipeline_options import GoogleCloudOptionsfrom apache_beam.options.pipeline_options import PipelineOptionsdef run(pipeline_options):    def parse_json_message(record):        logging.info("**************")        logging.info(record)        print(record)        print(type(record))        print(record[1])        print(type(record[1]))        logging.info("**************")        return record    with beam.Pipeline(runner="DirectRunner", options=pipeline_options) as pipeline:        schema = "name:STRING"        ride_col = (                pipeline                | ReadFromKafka(consumer_config={'bootstrap.servers': "pkc-lgk0v.us-west1.gcp.confluent.cloud:9092",                                                 "security.protocol": "SASL_SSL",                                                 "sasl.mechanism": "PLAIN",                                                 "sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username='POOXPGNP77VJ7HAM' password='mVdyY2TzK4xhr9pIiGGg3FngcsjWMMjRsPnI/eUoRGMAFv2iIC+e2heGUgQXGhnD';",                                                 'auto.offset.reset': 'latest',                                                 "session.timeout.ms": "45000",                                                 'default.api.timeout.ms': '300000',                                                 "group.id": "mongodb",                                                 "client.id": "connector-producer-lcc-68zdx6-0"                                                 },                                # commit_offset_in_finalize=True,                                # max_num_records=10,                                # max_read_time=10,                                # with_metadata=True,                                topics=["mjtestm40.mjtest.oos"])            )        # _ = (        #         ride_col        #         | WriteToBigQuery(table="focal-backend:sd_store_test_mj.mjtestm40", schema=schema))if __name__ == '__main__':    logging.getLogger().setLevel(logging.INFO)    import argparse    parser = argparse.ArgumentParser()    known_args, pipeline_args = parser.parse_known_args()    pipeline_options = PipelineOptions(        pipeline_args, save_main_session=True, streaming=True,        direct_num_workers=1, experiments=["use_deprecated_read"]    )    project = pipeline_options.view_as(GoogleCloudOptions).project    if project is None:        parser.print_usage()        print(sys.argv[0] + ': error: argument --project is required')        sys.exit(1)    run(pipeline_options=pipeline_options)