## Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements.  See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the "License"); you may not use this file except in compliance with# the License.  You may obtain a copy of the License at##    http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.#"""An example that writes to and reads from Kafka. This example reads from the PubSub NYC Taxi stream described in https://github.com/googlecodelabs/cloud-dataflow-nyc-taxi-tycoon, writes to a given Kafka topic and reads back from the same Kafka topic. """# pytype: skip-fileimport loggingimport sysimport apache_beam as beamfrom apache_beam.io.kafka import ReadFromKafkafrom apache_beam.io import WriteToBigQueryfrom apache_beam.options.pipeline_options import GoogleCloudOptionsfrom apache_beam.options.pipeline_options import PipelineOptionsdef run(pipeline_options):    def parse_json_message(record):        logging.info("**************")        logging.info(record)        print(record)        print(type(record))        print(record[1])        print(type(record[1]))        logging.info("**************")        return record    with beam.Pipeline(options=pipeline_options) as pipeline:        schema = "name:STRING"        ride_col = (                pipeline                | ReadFromKafka(consumer_config={'bootstrap.servers': "pkc-lgk0v.us-west1.gcp.confluent.cloud:9092",                                                 "security.protocol": "SASL_SSL",                                                 "sasl.mechanism": "PLAIN",                                                 "sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username='POOXPGNP77VJ7HAM' password='mVdyY2TzK4xhr9pIiGGg3FngcsjWMMjRsPnI/eUoRGMAFv2iIC+e2heGUgQXGhnD';",                                                 'auto.offset.reset': 'latest',                                                 "session.timeout.ms": "45000",                                                 "group.id": "mongodb",                                                 "client.id": "connector-producer-lcc-68zdx6-0"},                                topics=["mjtestm40.mjtest.oos"])                | beam.Map(lambda record: parse_json_message(record))                | "print" >> beam.Map(print)            )        _ = (                ride_col                | WriteToBigQuery(table="focal-backend:sd_store_test_mj.mjtestm40", schema=schema))if __name__ == '__main__':    logging.getLogger().setLevel(logging.INFO)    import argparse    parser = argparse.ArgumentParser()    known_args, pipeline_args = parser.parse_known_args()    pipeline_options = PipelineOptions(        pipeline_args, save_main_session=True, streaming=True)    project = pipeline_options.view_as(GoogleCloudOptions).project    if project is None:        parser.print_usage()        print(sys.argv[0] + ': error: argument --project is required')        sys.exit(1)    run(pipeline_options=pipeline_options)