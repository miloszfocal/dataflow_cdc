# import logging# import sys## import apache_beam as beam# from apache_beam.io.kafka import ReadFromKafka# from apache_beam.io import WriteToBigQuery# from apache_beam.options.pipeline_options import GoogleCloudOptions# from apache_beam.options.pipeline_options import PipelineOptions## def run(argv=None):###     pipeline_options = PipelineOptions(pipeline_args)## kafka_options = {'bootstrap.servers': known_args.bootstrap,#                  'client.id': 'di-qa-fraud-beam',#                  'group.id': 'di-qa-fraud-beam',#                  'auto.offset.reset': 'earliest'}# if known_args.is_sasl_enabled:#     kafka_options['sasl.mechanism'] = 'PLAIN'# kafka_options['security.protocol'] = 'SASL_SSL'# kafka_options['sasl.jaas.config'] =# f"org.apache.kafka.common.security.plain.PlainLoginModule required# username =\"{known_args.sasl_username}\"# password =\"{known_args.sasl_password}\";"# if known_args.incidents_api_url:#     os.environ["INCIDENTS_API_URI"] = known_args.incidents_api_url# # Direct runner and Flink runner are not yet well supporting Kafka# streaming# mode# # see https://issues.apache.org/jira/browse/BEAM-11991# # https://issues.apache.org/jira/browse/BEAM-11993# # https://issues.apache.org/jira/browse/BEAM-11998# # in dev mode, only consume 1 record before ending the pipeline# max_num_records = 1 if known_args.dev else None# max_num_records = None# incident_hook = IncidentsHook(debug=False)# logging.debug('debug message')# logging.info('info message')# with beam.Pipeline(options=pipeline_options) as p:#     (#             p#             | 'Read From Kafka' >>#             ReadFromKafka(consumer_config=kafka_options,#                           topics=[_QA_TOPIC],##                           start_read_time=1624143600000,#                           max_num_records=10,##                           commit_offset_in_finalize=True)#             | 'Jus log' >> beam.ParDo(PrintFn())##     )